{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7871c877",
   "metadata": {},
   "source": [
    "\n",
    "# PushT Discretization & Codebook Notebook (K=9, K=16)\n",
    "\n",
    "This notebook:\n",
    "1. Loads **relative actions** (`rel_actions.pth`, shape `(E, T_max, 2)`) and **sequence lengths** (`seq_lengths.pkl`).\n",
    "2. Fits **MiniBatchKMeans** codebooks on the **train** split for **K = 9** and **K = 16**.\n",
    "3. Saves codebooks as `action_codebook_k9.npy` and `action_codebook_k16.npy` in the train directory.\n",
    "4. Discretizes **train** and **val** splits to `rel_actions_discrete_9.pth` and `rel_actions_discrete_16.pth` using **float labels in [0..K-1]** (both channels set to the label, padding set to **0.0**).\n",
    "5. Provides a **filtering/analysis** cell to measure effects of removing episodes with outlier actions (does **not** change training data unless you explicitly use it).\n",
    "6. Performs **sanity checks** on outputs (shape, label range, entropy, quick QE sample).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701df5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Config & Imports ===\n",
    "import os, math, gc, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import psutil\n",
    "\n",
    "# ---- EDIT THESE to your dataset layout ----\n",
    "TRAIN_DIR = Path(os.getenv(\"TRAIN_DIR\", \"data/pusht/train\"))  # must contain rel_actions.pth and seq_lengths.pkl\n",
    "VAL_DIR   = Path(os.getenv(\"VAL_DIR\",   \"data/pusht/val\"))\n",
    "\n",
    "# K choices\n",
    "K_LIST = [9, 15]\n",
    "SEED = 0\n",
    "\n",
    "# File names (expected existing inputs)\n",
    "ACTIONS_FNAME = \"rel_actions.pth\"     # shape (E, T_max, 2), float\n",
    "LENGTHS_FNAME = \"seq_lengths.pkl\"     # list/array of ints length E\n",
    "\n",
    "# Output names\n",
    "DISCRETE_TEMPLATE = \"rel_actions_discrete_{K}.pth\"  # same shape as actions, stores float labels in both channels (padding=0.0)\n",
    "CODEBOOK_TEMPLATE = \"action_codebook_k{K}.npy\"      # (K, 2) float32\n",
    "\n",
    "# Safety\n",
    "torch.set_grad_enabled(False)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"TRAIN_DIR = {TRAIN_DIR.resolve()}\")\n",
    "print(f\"VAL_DIR   = {VAL_DIR.resolve()}\")\n",
    "print(f\"K_LIST    = {K_LIST}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Utilities ===\n",
    "def load_split(split_dir: Path):\n",
    "    actions = torch.load(split_dir / ACTIONS_FNAME, map_location=\"cpu\")  # (E, T_max, 2)\n",
    "    with open(split_dir / LENGTHS_FNAME, \"rb\") as f:\n",
    "        lengths = np.asarray(pickle.load(f), dtype=np.int64)            # (E,)\n",
    "    assert actions.ndim == 3 and actions.shape[-1] == 2, \"rel_actions must be (E, T_max, 2)\"\n",
    "    E, T_max, _ = actions.shape\n",
    "    assert lengths.shape[0] == E, f\"seq_lengths length ({lengths.shape[0]}) != num episodes ({E})\"\n",
    "    return actions, lengths\n",
    "\n",
    "def flatten_valid(actions_3d: torch.Tensor, lengths: np.ndarray) -> np.ndarray:\n",
    "    \\\"\\\"\\\"Concatenate only valid (unpadded) steps into (N,2) float32.\\\"\\\"\\\"\n",
    "    E, T_max, _ = actions_3d.shape\n",
    "    lst = []\n",
    "    for i in range(E):\n",
    "        L = int(lengths[i])\n",
    "        if L > 0:\n",
    "            lst.append(actions_3d[i, :L, :].cpu().numpy())\n",
    "    if not lst:\n",
    "        return np.empty((0, 2), dtype=np.float32)\n",
    "    flat = np.concatenate(lst, axis=0).astype(np.float32, copy=False)\n",
    "    return flat\n",
    "\n",
    "def fit_codebook_kmeans(xy_flat: np.ndarray, K: int, seed: int = 0) -> np.ndarray:\n",
    "    \\\"\\\"\\\"Fit MiniBatchKMeans codebook on flattened valid actions.\\\"\\\"\\\"\n",
    "    assert xy_flat.ndim == 2 and xy_flat.shape[1] == 2 and xy_flat.size > 0\n",
    "    bs = 2048  # minibatch size\n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=K, random_state=seed, batch_size=bs, n_init=\"auto\"\n",
    "    )\n",
    "    kmeans.fit(xy_flat)\n",
    "    centers = kmeans.cluster_centers_.astype(np.float32, copy=False)\n",
    "    return centers\n",
    "\n",
    "def predict_labels_chunked(xy_flat: np.ndarray, centers: np.ndarray, chunk_size: int = None) -> np.ndarray:\n",
    "    \\\"\\\"\\\"Argmin over squared distances to centers, chunked to avoid RAM blowups. Returns labels int64 shape (N,).\\\"\\\"\\\"\n",
    "    N, D = xy_flat.shape\n",
    "    K = centers.shape[0]\n",
    "    if chunk_size is None:\n",
    "        # memory-based heuristic: keep approx <= 400MB buffer for distances\n",
    "        avail_gb = psutil.virtual_memory().available / 1e9\n",
    "        target_bytes = min(400, max(100, int(avail_gb * 0.5))) * 1024**2  # between 100MB and ~0.5*avail\n",
    "        bytes_per_row = K * 4  # float32 distances\n",
    "        chunk_size = max(100_000, min(N, target_bytes // bytes_per_row))\n",
    "    labels = np.empty(N, dtype=np.int64)\n",
    "    c2 = (centers ** 2).sum(axis=1)  # (K,)\n",
    "    for s in range(0, N, chunk_size):\n",
    "        e = min(N, s + chunk_size)\n",
    "        X = xy_flat[s:e]  # (M,2)\n",
    "        x2 = (X ** 2).sum(axis=1, keepdims=True)  # (M,1)\n",
    "        dots = X @ centers.T                       # (M,K)\n",
    "        d2 = x2 + c2[None, :] - 2.0 * dots\n",
    "        labels[s:e] = np.argmin(d2, axis=1)\n",
    "    return labels\n",
    "\n",
    "def labels_to_tensor_like_floatonly(actions_3d: torch.Tensor, lengths: np.ndarray, labels: np.ndarray) -> torch.Tensor:\n",
    "    \\\"\\\"\\\"Map flat labels back to (E, T_max, 2) with float labels only (both channels). Padding -> 0.0.\\\"\\\"\\\"\n",
    "    E, T_max, _ = actions_3d.shape\n",
    "    out = torch.zeros_like(actions_3d)  # default padding = 0.0 in both channels\n",
    "    idx = 0\n",
    "    for i in range(E):\n",
    "        L = int(lengths[i])\n",
    "        if L > 0:\n",
    "            l_slice = labels[idx: idx + L].astype(np.float32)\n",
    "            idx += L\n",
    "            out[i, :L, 0] = torch.from_numpy(l_slice)\n",
    "            out[i, :L, 1] = torch.from_numpy(l_slice)\n",
    "    assert idx == labels.shape[0], \"Label count mismatch when reshaping\"\n",
    "    return out\n",
    "\n",
    "def summarize_labels(labels: np.ndarray, K: int, name: str):\n",
    "    from collections import Counter\n",
    "    cnt = Counter(labels.tolist())\n",
    "    total = labels.size\n",
    "    if total > 0:\n",
    "        entropy = -sum((n/total)*math.log2(n/total) for n in cnt.values() if n > 0)\n",
    "    else:\n",
    "        entropy = 0.0\n",
    "    top = sorted(cnt.items(), key=lambda kv: kv[1], reverse=True)[:10]\n",
    "    print(f\\\"[{name}] K={K} | N={total:,} | Entropy={entropy:.2f} bits | Unique labels={len(cnt)}/{K}\\\")\n",
    "    print(\\\" Top-10 label counts:\\\", top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0c8ff",
   "metadata": {},
   "source": [
    "### Optional: Episode filtering analysis (does not affect training unless you use the masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c18e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Optional: Filter episodes using EPISODES_LENGTHS_PATH and report stats ===\n",
    "import pandas as pd\n",
    "\n",
    "# Config for this analysis cell only (does not modify training data)\n",
    "USE_PERCENTILE = True\n",
    "PERCENTILE     = 99.0\n",
    "ABS_THRESHOLD  = 60.0\n",
    "\n",
    "# Pick which split to analyze (train by default)\n",
    "SPLIT_DIR = TRAIN_DIR\n",
    "\n",
    "# Allow overriding with EPISODES_LENGTHS_PATH, otherwise use split lengths\n",
    "if 'EPISODES_LENGTHS_PATH' in globals():\n",
    "    with open(EPISODES_LENGTHS_PATH, \"rb\") as f:\n",
    "        ep_lengths = np.asarray(pickle.load(f), dtype=np.int64)\n",
    "    actions_3d = torch.load(SPLIT_DIR / ACTIONS_FNAME, map_location=\"cpu\")\n",
    "else:\n",
    "    actions_3d, ep_lengths = load_split(SPLIT_DIR)\n",
    "\n",
    "E, T_max, _ = actions_3d.shape\n",
    "flat = flatten_valid(actions_3d, ep_lengths)\n",
    "mag = np.linalg.norm(flat, axis=1)\n",
    "\n",
    "if USE_PERCENTILE:\n",
    "    threshold = float(np.percentile(mag, PERCENTILE))\n",
    "    th_desc = f\">{PERCENTILE:.2f}p ({threshold:.2f}px)\"\n",
    "else:\n",
    "    threshold = float(ABS_THRESHOLD)\n",
    "    th_desc = f\">{threshold:.2f}px\"\n",
    "\n",
    "starts = np.r_[0, np.cumsum(ep_lengths)[:-1]]\n",
    "ends   = starts + ep_lengths\n",
    "\n",
    "ep_max = np.empty(E, dtype=np.float32)\n",
    "idx = 0\n",
    "for i, (s, e) in enumerate(zip(starts, ends)):\n",
    "    L = (e - s)\n",
    "    ep_max[i] = mag[idx: idx + L].max() if L > 0 else 0.0\n",
    "    idx += L\n",
    "assert idx == flat.shape[0]\n",
    "\n",
    "keep_episode_mask = ep_max <= threshold\n",
    "kept_E = int(keep_episode_mask.sum())\n",
    "\n",
    "# Build action mask for kept episodes\n",
    "keep_action_mask = np.zeros(flat.shape[0], dtype=bool)\n",
    "idx = 0\n",
    "for i, (s, e, keep) in enumerate(zip(starts, ends, keep_episode_mask)):\n",
    "    L = (e - s)\n",
    "    if keep and L > 0:\n",
    "        keep_action_mask[idx: idx + L] = True\n",
    "    idx += L\n",
    "kept_N = int(keep_action_mask.sum())\n",
    "\n",
    "outlier_action_mask = mag > threshold\n",
    "outlier_N = int(outlier_action_mask.sum())\n",
    "\n",
    "summary = {\n",
    "    \"Split\": SPLIT_DIR.name,\n",
    "    \"Threshold\": th_desc,\n",
    "    \"Episodes (total)\": f\"{E:,}\",\n",
    "    \"Episodes kept\": f\"{kept_E:,} ({kept_E/E*100:.2f}%)\",\n",
    "    \"Actions (total)\": f\"{flat.shape[0]:,}\",\n",
    "    \"Actions kept\": f\"{kept_N:,} ({kept_N/flat.shape[0]*100:.2f}%)\",\n",
    "    \"Outlier actions\": f\"{outlier_N:,} ({outlier_N/flat.shape[0]*100:.2f}%)\",\n",
    "}\n",
    "display(pd.DataFrame([summary]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38395e57",
   "metadata": {},
   "source": [
    "### Fit codebooks on **train** and save centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Fit codebooks on TRAIN valid steps and save centroids ===\n",
    "train_actions, train_lengths = load_split(TRAIN_DIR)\n",
    "flat_train = flatten_valid(train_actions, train_lengths)\n",
    "print(f\"[Train] Valid actions: {flat_train.shape[0]:,}\")\n",
    "\n",
    "CODEBOOK_PATHS = {}\n",
    "\n",
    "for K in K_LIST:\n",
    "    print(f\"\\n-- Fitting MiniBatchKMeans K={K} --\")\n",
    "    centers = fit_codebook_kmeans(flat_train, K, seed=SEED)\n",
    "    np.save(TRAIN_DIR / CODEBOOK_TEMPLATE.format(K=K), centers)\n",
    "    CODEBOOK_PATHS[K] = TRAIN_DIR / CODEBOOK_TEMPLATE.format(K=K)\n",
    "    print(f\" Saved codebook to: {CODEBOOK_PATHS[K]}\")\n",
    "    # quick reconstruction error on a 100k sample for visibility\n",
    "    n_sample = min(100_000, len(flat_train))\n",
    "    idx = np.random.choice(len(flat_train), size=n_sample, replace=False) if n_sample > 0 else np.array([], dtype=int)\n",
    "    if n_sample > 0:\n",
    "        sample = flat_train[idx]\n",
    "        labels_sample = predict_labels_chunked(sample, centers, chunk_size=200_000)\n",
    "        err = np.linalg.norm(sample - centers[labels_sample], axis=1)\n",
    "        print(f\" Sample mean error: {err.mean():.2f}px | p95: {np.percentile(err,95):.2f}px\")\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36559eb9",
   "metadata": {},
   "source": [
    "### Discretize **train** and **val** using the fitted codebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Discretize train and val with fitted codebooks; save rel_actions_discrete_{K}.pth ===\n",
    "def discretize_split_floatlabels(split_dir: Path, K: int, centers: np.ndarray):\n",
    "    actions, lengths = load_split(split_dir)\n",
    "    flat = flatten_valid(actions, lengths)\n",
    "    print(f\"[{split_dir.name}] Discretizing with K={K} | valid steps={flat.shape[0]:,}\")\n",
    "    labels = predict_labels_chunked(flat, centers) if flat.size > 0 else np.empty((0,), dtype=np.int64)\n",
    "    summarize_labels(labels, K, name=f\"{split_dir.name}\")\n",
    "    out_tensor = labels_to_tensor_like_floatonly(actions, lengths, labels)\n",
    "    out_path = split_dir / DISCRETE_TEMPLATE.format(K=K)\n",
    "    torch.save(out_tensor, out_path)\n",
    "    print(f\" Saved: {out_path} | shape={tuple(out_tensor.shape)} | dtype={out_tensor.dtype}\")\n",
    "    return out_path\n",
    "\n",
    "SAVED_DISCRETE = {}\n",
    "for K in K_LIST:\n",
    "    centers = np.load(CODEBOOK_PATHS[K])\n",
    "    print(f\"\\n== K={K} ==\")\n",
    "    SAVED_DISCRETE[(K, \"train\")] = discretize_split_floatlabels(TRAIN_DIR, K, centers)\n",
    "    SAVED_DISCRETE[(K, \"val\")]   = discretize_split_floatlabels(VAL_DIR,   K, centers)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33af9cc",
   "metadata": {},
   "source": [
    "### Post-checks: shapes, label ranges, integer-like labels, quick QE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be92594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quick_check_floatlabels(split_dir: Path, K: int, centers: np.ndarray, name: str):\n",
    "    disc_t = torch.load(split_dir / DISCRETE_TEMPLATE.format(K=K), map_location=\"cpu\")  # (E,T,2)\n",
    "    acts_t = torch.load(split_dir / ACTIONS_FNAME,              map_location=\"cpu\")\n",
    "    with open(split_dir / LENGTHS_FNAME, \"rb\") as f:\n",
    "        lengths = np.asarray(pickle.load(f), dtype=np.int64)\n",
    "\n",
    "    E, T_max, _ = acts_t.shape\n",
    "    assert disc_t.shape == acts_t.shape == (E, T_max, 2)\n",
    "    # Collect valid labels (first channel)\n",
    "    labels = []\n",
    "    for i in range(E):\n",
    "        L = int(lengths[i])\n",
    "        if L > 0:\n",
    "            labels.append(disc_t[i, :L, 0].numpy())\n",
    "    labels = np.concatenate(labels, axis=0).astype(np.float32) if labels else np.empty((0,), dtype=np.float32)\n",
    "\n",
    "    if labels.size > 0:\n",
    "        vmin, vmax = float(labels.min()), float(labels.max())\n",
    "        in_range = (vmin >= 0.0) and (vmax <= (K - 1 + 1e-6))\n",
    "        labels_rounded = np.rint(labels).astype(np.int64)\n",
    "        integer_ok = np.allclose(labels, labels_rounded, atol=1e-6)\n",
    "        summarize_labels(labels_rounded, K, name=f\"{name}\")\n",
    "\n",
    "        print(f\"[{name}] K={K} | shape OK: {disc_t.shape} | values in [0,{K-1}]: {in_range} \"\n",
    "              f\"| integer-like: {integer_ok} | N_valid={labels.size:,} | min={vmin:.1f} max={vmax:.1f}\")\n",
    "\n",
    "        # Quick QE on a sample of original continuous actions\n",
    "        flat = flatten_valid(acts_t, lengths)\n",
    "        n_sample = min(100_000, len(flat))\n",
    "        if n_sample > 0:\n",
    "            idx = np.random.choice(len(flat), size=n_sample, replace=False)\n",
    "            sample = flat[idx]\n",
    "            pred = predict_labels_chunked(sample, centers, chunk_size=200_000)\n",
    "            err = np.linalg.norm(sample - centers[pred], axis=1)\n",
    "            print(f\" QE(sample): mean={err.mean():.2f}px | p95={np.percentile(err,95):.2f}px\")\n",
    "    else:\n",
    "        print(f\"[{name}] K={K} | No valid labels found.\")\n",
    "\n",
    "for K in K_LIST:\n",
    "    centers = np.load(CODEBOOK_PATHS[K])\n",
    "    print(f\"\\n== Post-checks for K={K} with float-only labels ==\")\n",
    "    quick_check_floatlabels(TRAIN_DIR, K, centers, name=\"train\")\n",
    "    quick_check_floatlabels(VAL_DIR,   K, centers, name=\"val\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
