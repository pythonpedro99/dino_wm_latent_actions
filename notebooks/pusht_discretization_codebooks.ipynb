{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PushT Noise discretization & dataset variants\n",
    "\n",
    "This notebook automates the PushT action discretization pipeline:\n",
    "\n",
    "1. Copy the original `pusht_noise` dataset into a new folder that will hold all discretized assets.\n",
    "2. Fit MiniBatchKMeans codebooks (`K=9` and `K=15`) on the train split and build a deterministic `r=24` fixed-compass codebook (center + 8 compass directions).\n",
    "3. Discretize `rel_actions.pth` for every split into `rel_actions_discretized_<variant>.pth` tensors that keep the original `(E, T_max, 2)` shape but store centroid vectors instead of the continuous actions.\n",
    "4. Add a `states_constant.pth` companion file (zeros with the same shape/dtype as `states.pth`) for non-proprio training tricks.\n",
    "5. Materialize three full datasets on disk: the base `pusht_noise_discretized` copy, plus two uniformly subsampled variants (`pusht_noise_discretized_1k` and `_10k`) built from random trajectories.\n",
    "6. Save all fitted codebooks under `codebooks/` to make downstream reuse trivial.\n",
    "\n",
    "Configure the source/destination directories via the environment variables `SRC_DATASET_DIR` or `DATASET_DIR`, and `DST_DATASET_DIR` if you do not want the notebook to default to `<DATASET_DIR>/pusht_noise` and `<DATASET_DIR>/pusht_noise_discretized`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078838e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DATASET_DIR\"] = \"/Users/julianquast/Documents/Documents - pythonPedro (Mac)/Bachelor Thesis/Datasets/pusht_noise\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports & global config ===\n",
    "import os, gc, math, json, pickle, shutil, re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import psutil\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "SPLITS = (\"train\", \"val\")\n",
    "ACTIONS_FNAME = \"rel_actions.pth\"\n",
    "LENGTHS_FNAME = \"seq_lengths.pkl\"\n",
    "STATES_FNAME = \"states.pth\"\n",
    "STATE_CONSTANT_FNAME = \"states_constant.pth\"\n",
    "\n",
    "VARIANTS = {\n",
    "    \"kmeans_k9\": {\"type\": \"kmeans\", \"K\": 9},\n",
    "    \"kmeans_k15\": {\"type\": \"kmeans\", \"K\": 15},\n",
    "    \"fixed_compass_r24\": {\"type\": \"fixed_compass\", \"r\": 24.0, \"n_dirs\": 4, \"include_center\": True},\n",
    "}\n",
    "\n",
    "SUBSET_SPECS = {\n",
    "    \"1k\": 1_000,\n",
    "    \"10k\": 10_000,\n",
    "}\n",
    "\n",
    "RANDOM_SEED = int(os.getenv(\"DISCRETIZATION_SEED\", \"0\"))\n",
    "\n",
    "\n",
    "def _resolve_dataset_dir(path_str: str, default_leaf: str = \"pusht_noise\") -> Path:\n",
    "    base = Path(path_str).expanduser()\n",
    "    if (base / \"train\").exists() and (base / \"val\").exists():\n",
    "        return base\n",
    "    candidate = base / default_leaf\n",
    "    if (candidate / \"train\").exists() and (candidate / \"val\").exists():\n",
    "        return candidate\n",
    "    raise FileNotFoundError(f\"Could not resolve dataset directory from {path_str!r}. Provide SRC_DATASET_DIR or DATASET_DIR.\")\n",
    "\n",
    "src_root_env = os.getenv(\"SRC_DATASET_DIR\", os.getenv(\"DATASET_DIR\", \"data\"))\n",
    "SRC_DATASET_DIR = _resolve_dataset_dir(src_root_env)\n",
    "\n",
    "dst_root_env = os.getenv(\"DST_DATASET_DIR\", str(SRC_DATASET_DIR.parent / \"pusht_noise_discretized\"))\n",
    "DST_DATASET_DIR = Path(dst_root_env).expanduser()\n",
    "\n",
    "print(f\"Source dataset : {SRC_DATASET_DIR}\")\n",
    "print(f\"Discretized dir: {DST_DATASET_DIR}\")\n",
    "print(f\"Variants       : {list(VARIANTS.keys())}\")\n",
    "print(f\"Subset specs   : {SUBSET_SPECS}\")\n",
    "print(f\"Random seed    : {RANDOM_SEED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf56611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Discretization helpers ===\n",
    "def load_actions_and_lengths(split_dir: Path):\n",
    "    actions = torch.load(split_dir / ACTIONS_FNAME, map_location=\"cpu\")\n",
    "    with open(split_dir / LENGTHS_FNAME, \"rb\") as f:\n",
    "        lengths = np.asarray(pickle.load(f), dtype=np.int64)\n",
    "    assert actions.shape[0] == len(lengths), f\"Mismatch: {actions.shape[0]} tensors vs {len(lengths)} lengths\"\n",
    "    return actions, lengths\n",
    "\n",
    "\n",
    "def load_lengths_only(split_dir: Path) -> np.ndarray:\n",
    "    with open(split_dir / LENGTHS_FNAME, \"rb\") as f:\n",
    "        lengths = np.asarray(pickle.load(f), dtype=np.int64)\n",
    "    return lengths\n",
    "\n",
    "\n",
    "def flatten_valid(actions_3d: torch.Tensor, lengths: np.ndarray) -> np.ndarray:\n",
    "    E, T_max, _ = actions_3d.shape\n",
    "    chunks: List[np.ndarray] = []\n",
    "    for epi in range(E):\n",
    "        L = int(lengths[epi])\n",
    "        if L > 0:\n",
    "            chunks.append(actions_3d[epi, :L].cpu().numpy())\n",
    "    if not chunks:\n",
    "        return np.empty((0, 2), dtype=np.float32)\n",
    "    flat = np.concatenate(chunks, axis=0).astype(np.float32, copy=False)\n",
    "    return flat\n",
    "\n",
    "\n",
    "def fit_codebook_kmeans(xy_flat: np.ndarray, K: int, seed: int) -> np.ndarray:\n",
    "    assert xy_flat.ndim == 2 and xy_flat.shape[1] == 2 and xy_flat.size > 0\n",
    "    batch_size = min(8192, max(2048, xy_flat.shape[0] // 32))\n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=K,\n",
    "        random_state=seed,\n",
    "        batch_size=batch_size,\n",
    "        n_init=\"auto\",\n",
    "    )\n",
    "    kmeans.fit(xy_flat)\n",
    "    centers = kmeans.cluster_centers_.astype(np.float32, copy=False)\n",
    "    return centers\n",
    "\n",
    "\n",
    "def predict_labels_chunked(xy_flat: np.ndarray, centers: np.ndarray, chunk_size: int | None = None) -> np.ndarray:\n",
    "    N = xy_flat.shape[0]\n",
    "    if N == 0:\n",
    "        return np.empty((0,), dtype=np.int64)\n",
    "    K = centers.shape[0]\n",
    "    if chunk_size is None:\n",
    "        avail_bytes = psutil.virtual_memory().available\n",
    "        bytes_per_row = K * 4  # float32 distances\n",
    "        target = max(100_000, min(N, (avail_bytes // 8) // max(bytes_per_row, 1)))\n",
    "        chunk_size = int(target)\n",
    "    labels = np.empty(N, dtype=np.int64)\n",
    "    center_norm = (centers ** 2).sum(axis=1)\n",
    "    for start in range(0, N, chunk_size):\n",
    "        end = min(N, start + chunk_size)\n",
    "        block = xy_flat[start:end]\n",
    "        block_norm = (block ** 2).sum(axis=1, keepdims=True)\n",
    "        dots = block @ centers.T\n",
    "        d2 = block_norm + center_norm[None, :] - 2.0 * dots\n",
    "        labels[start:end] = np.argmin(d2, axis=1)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def map_labels_to_centroids(actions_3d: torch.Tensor, lengths: np.ndarray, labels: np.ndarray, centers: np.ndarray) -> torch.Tensor:\n",
    "    out = torch.zeros_like(actions_3d)\n",
    "    idx = 0\n",
    "    for epi in range(actions_3d.shape[0]):\n",
    "        L = int(lengths[epi])\n",
    "        if L > 0:\n",
    "            lab_slice = labels[idx: idx + L]\n",
    "            idx += L\n",
    "            centroid_slice = torch.from_numpy(centers[lab_slice]).to(out.dtype)\n",
    "            out[epi, :L] = centroid_slice\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_fixed_compass(r: float, n_dirs: int = 8, include_center: bool = True) -> np.ndarray:\n",
    "    assert n_dirs > 0, \"n_dirs must be positive\"\n",
    "    dirs = np.linspace(0.0, 2.0 * math.pi, n_dirs, endpoint=False)\n",
    "    vecs = np.stack([np.cos(dirs), np.sin(dirs)], axis=1).astype(np.float32) * float(r)\n",
    "    if include_center:\n",
    "        centers = np.vstack([np.zeros((1, 2), dtype=np.float32), vecs])\n",
    "    else:\n",
    "        centers = vecs\n",
    "    return centers.astype(np.float32)\n",
    "\n",
    "\n",
    "def summarize_labels(labels: np.ndarray, K: int, name: str):\n",
    "    if labels.size == 0:\n",
    "        print(f\"[{name}] No labels to summarize.\")\n",
    "        return\n",
    "    counts = np.bincount(labels, minlength=K)\n",
    "    probs = counts / counts.sum()\n",
    "    entropy = -(probs[probs > 0] * np.log2(probs[probs > 0])).sum()\n",
    "    print(f\"[{name}] usage -> min={counts.min()} max={counts.max()} entropy={entropy:.3f} bits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b901da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Copy dataset & gather metadata ===\n",
    "def ensure_dataset_copy(src: Path, dst: Path):\n",
    "    if dst.exists():\n",
    "        print(f\"[skip] {dst} already exists\")\n",
    "        return\n",
    "    print(f\"Copying dataset tree from {src} -> {dst} ...\")\n",
    "    shutil.copytree(src, dst)\n",
    "    print(\"Copy complete.\")\n",
    "\n",
    "\n",
    "ensure_dataset_copy(SRC_DATASET_DIR, DST_DATASET_DIR)\n",
    "\n",
    "split_meta: Dict[str, Dict[str, np.ndarray | int]] = {}\n",
    "total_eps = 0\n",
    "for split in SPLITS:\n",
    "    split_dir = SRC_DATASET_DIR / split\n",
    "    lengths = load_lengths_only(split_dir)\n",
    "    split_meta[split] = {\n",
    "        \"lengths\": lengths,\n",
    "        \"num_episodes\": int(len(lengths)),\n",
    "        \"max_T\": int(lengths.max()) if lengths.size else 0,\n",
    "    }\n",
    "    total_eps += len(lengths)\n",
    "    print(f\"[{split}] episodes={len(lengths):,} | max_T={split_meta[split]['max_T']}\")\n",
    "\n",
    "print(f\"Total episodes across splits: {total_eps:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9290b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fit codebooks (MiniBatchKMeans + fixed compass) ===\n",
    "CODEBOOK_DIR = DST_DATASET_DIR / \"codebooks\"\n",
    "CODEBOOK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_actions, train_lengths = load_actions_and_lengths(SRC_DATASET_DIR / \"train\")\n",
    "flat_train = flatten_valid(train_actions, train_lengths)\n",
    "print(f\"Train valid steps: {flat_train.shape[0]:,}\")\n",
    "\n",
    "discretization_centers: Dict[str, np.ndarray] = {}\n",
    "codebook_paths: Dict[str, Path] = {}\n",
    "\n",
    "for name, cfg in VARIANTS.items():\n",
    "    if cfg[\"type\"] == \"kmeans\":\n",
    "        centers = fit_codebook_kmeans(flat_train, cfg[\"K\"], seed=RANDOM_SEED)\n",
    "        summarize_labels(\n",
    "            predict_labels_chunked(flat_train[: min(200_000, len(flat_train))], centers),\n",
    "            centers.shape[0],\n",
    "            name + \"_sample\",\n",
    "        )\n",
    "    elif cfg[\"type\"] == \"fixed_compass\":\n",
    "        centers = build_fixed_compass(cfg[\"r\"], n_dirs=cfg.get(\"n_dirs\", 8), include_center=cfg.get(\"include_center\", True))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown variant type: {cfg['type']}\")\n",
    "\n",
    "    out_path = CODEBOOK_DIR / f\"{name}_centroids.npy\"\n",
    "    np.save(out_path, centers)\n",
    "    discretization_centers[name] = centers\n",
    "    codebook_paths[name] = out_path\n",
    "    print(f\"Saved {name} codebook -> {out_path} | shape={centers.shape}\")\n",
    "\n",
    "# cleanup to free RAM\n",
    "train_actions = None\n",
    "flat_train = None\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90673e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Discretize rel_actions & add states_constant ===\n",
    "def ensure_states_constant(split_dir: Path):\n",
    "    out_path = split_dir / STATE_CONSTANT_FNAME\n",
    "    src_path = split_dir / STATES_FNAME\n",
    "    if out_path.exists():\n",
    "        print(f\"[skip] {out_path.name} already exists\")\n",
    "        return\n",
    "    states = torch.load(src_path, map_location=\"cpu\")\n",
    "    zeros = torch.zeros_like(states)\n",
    "    torch.save(zeros, out_path)\n",
    "    print(f\"Saved {out_path} (zeros with shape={tuple(states.shape)})\")\n",
    "    del states, zeros\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def discretize_split(split: str, variant_name: str, centers: np.ndarray):\n",
    "    src_split = SRC_DATASET_DIR / split\n",
    "    dst_split = DST_DATASET_DIR / split\n",
    "    actions, lengths = load_actions_and_lengths(src_split)\n",
    "    flat = flatten_valid(actions, lengths)\n",
    "    labels = predict_labels_chunked(flat, centers)\n",
    "    summarize_labels(labels, centers.shape[0], name=f\"{split}_{variant_name}\")\n",
    "    disc = map_labels_to_centroids(actions, lengths, labels, centers)\n",
    "    out_path = dst_split / f\"rel_actions_discretized_{variant_name}.pth\"\n",
    "    torch.save(disc, out_path)\n",
    "    print(f\"Saved {out_path} | shape={tuple(disc.shape)}\")\n",
    "    del actions, flat, labels, disc\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "for variant_name, centers in discretization_centers.items():\n",
    "    print(f\"\\n== Discretizing variant: {variant_name} ==\")\n",
    "    for split in SPLITS:\n",
    "        discretize_split(split, variant_name, centers)\n",
    "\n",
    "for split in SPLITS:\n",
    "    ensure_states_constant(DST_DATASET_DIR / split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74cc869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Verify discretized rel_actions files ===\n",
    "print(\"\\n== Checking discretized rel_actions artifacts ==\")\n",
    "for split in SPLITS:\n",
    "    print(f\"[{split}]\")\n",
    "    for variant in VARIANTS.keys():\n",
    "        rel_path = Path(split) / f\"rel_actions_discretized_{variant}.pth\"\n",
    "        abs_path = DST_DATASET_DIR / rel_path\n",
    "        if not abs_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing {rel_path} in {DST_DATASET_DIR}\")\n",
    "        size_mb = abs_path.stat().st_size / (1024 ** 2)\n",
    "        print(f\"  - {rel_path.name:>32} | size={size_mb:8.2f} MB\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Subset helpers (1k / 10k datasets) ===\n",
    "def allocate_subset_counts(total_target: int, split_meta: Dict[str, Dict[str, int]]) -> Dict[str, int]:\n",
    "    total_available = sum(meta[\"num_episodes\"] for meta in split_meta.values())\n",
    "    if total_target >= total_available:\n",
    "        return {split: meta[\"num_episodes\"] for split, meta in split_meta.items()}\n",
    "    counts: Dict[str, int] = {}\n",
    "    remaining = total_target\n",
    "    for idx, split in enumerate(SPLITS):\n",
    "        meta = split_meta[split]\n",
    "        if idx == len(SPLITS) - 1:\n",
    "            take = remaining\n",
    "        else:\n",
    "            prop = meta[\"num_episodes\"] / total_available\n",
    "            take = min(meta[\"num_episodes\"], int(round(total_target * prop)))\n",
    "        counts[split] = max(0, min(meta[\"num_episodes\"], take))\n",
    "        remaining -= counts[split]\n",
    "    while remaining > 0:\n",
    "        for split in SPLITS:\n",
    "            if counts[split] < split_meta[split][\"num_episodes\"]:\n",
    "                counts[split] += 1\n",
    "                remaining -= 1\n",
    "                if remaining == 0:\n",
    "                    break\n",
    "    return counts\n",
    "\n",
    "\n",
    "def subset_tensor_file(path: Path, idxs: np.ndarray, orig_count: int):\n",
    "    if not path.exists():\n",
    "        return False\n",
    "    obj = torch.load(path, map_location=\"cpu\")\n",
    "    if torch.is_tensor(obj) and obj.shape[0] == orig_count:\n",
    "        subset = obj[idxs]\n",
    "        torch.save(subset, path)\n",
    "        return True\n",
    "    if isinstance(obj, (list, tuple)) and len(obj) == orig_count:\n",
    "        subset = [obj[int(i)] for i in idxs]\n",
    "        torch.save(subset, path)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def subset_pickle_file(path: Path, idxs: np.ndarray, orig_count: int):\n",
    "    if not path.exists():\n",
    "        return False\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = data.tolist()\n",
    "    if isinstance(data, list) and len(data) == orig_count:\n",
    "        subset = [data[int(i)] for i in idxs]\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(subset, f)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def build_episode_file_map(obs_dir: Path) -> Dict[int, Path]:\n",
    "    mapping = {}\n",
    "    if not obs_dir.exists():\n",
    "        return mapping\n",
    "    pattern = re.compile(r\"episode_(\\d+)\")\n",
    "    for path in obs_dir.glob(\"episode_*\"):\n",
    "        m = pattern.search(path.stem)\n",
    "        if m:\n",
    "            mapping[int(m.group(1))] = path\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def subset_observation_media(obs_dir: Path, idxs: np.ndarray):\n",
    "    if not obs_dir.exists():\n",
    "        return\n",
    "    idxs = np.asarray(idxs, dtype=np.int64)\n",
    "    mapping = build_episode_file_map(obs_dir)\n",
    "    tmp_dir = obs_dir.parent / (obs_dir.name + \"_subset_tmp\")\n",
    "    if tmp_dir.exists():\n",
    "        shutil.rmtree(tmp_dir)\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pad = max(3, len(str(len(idxs))))\n",
    "    for new_idx, old_idx in enumerate(idxs):\n",
    "        src_path = mapping.get(int(old_idx))\n",
    "        if src_path is None:\n",
    "            raise FileNotFoundError(f\"Missing obs file for episode {old_idx} in {obs_dir}\")\n",
    "        dst_path = tmp_dir / f\"episode_{new_idx:0{pad}d}{src_path.suffix}\"\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "    shutil.rmtree(obs_dir)\n",
    "    tmp_dir.rename(obs_dir)\n",
    "\n",
    "\n",
    "def subset_split_dir(split_dir: Path, idxs: np.ndarray, orig_count: int):\n",
    "    idxs = np.asarray(idxs, dtype=np.int64)\n",
    "    for path in split_dir.glob(\"*.pth\"):\n",
    "        subset_tensor_file(path, idxs, orig_count)\n",
    "    for path in split_dir.glob(\"*.pt\"):\n",
    "        subset_tensor_file(path, idxs, orig_count)\n",
    "    for path in split_dir.glob(\"*.pkl\"):\n",
    "        subset_pickle_file(path, idxs, orig_count)\n",
    "    subset_observation_media(split_dir / \"obses\", idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build subset datasets (1k / 10k trajectories) ===\n",
    "subset_roots: Dict[str, Path] = {}\n",
    "for subset_name, target_total in SUBSET_SPECS.items():\n",
    "    subset_root = DST_DATASET_DIR.parent / f\"{DST_DATASET_DIR.name}_{subset_name}\"\n",
    "    subset_roots[subset_name] = subset_root\n",
    "    if subset_root.exists():\n",
    "        print(f\"[skip] {subset_root} already exists\")\n",
    "        continue\n",
    "    print(f\"\\n== Building subset {subset_name} ({target_total} trajectories) ==\")\n",
    "    shutil.copytree(DST_DATASET_DIR, subset_root)\n",
    "    counts = allocate_subset_counts(target_total, split_meta)\n",
    "    subset_rng = np.random.default_rng(RANDOM_SEED + target_total)\n",
    "    for split in SPLITS:\n",
    "        orig_count = split_meta[split][\"num_episodes\"]\n",
    "        take = min(orig_count, counts.get(split, 0))\n",
    "        if take <= 0:\n",
    "            idxs = np.empty((0,), dtype=np.int64)\n",
    "        elif take == orig_count:\n",
    "            idxs = np.arange(orig_count, dtype=np.int64)\n",
    "        else:\n",
    "            idxs = np.sort(subset_rng.choice(orig_count, size=take, replace=False))\n",
    "        print(f\"[{subset_name}/{split}] keeping {len(idxs)} / {orig_count} episodes\")\n",
    "        subset_split_dir(subset_root / split, idxs, orig_count)\n",
    "    print(f\"Subset {subset_name} written to {subset_root}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
