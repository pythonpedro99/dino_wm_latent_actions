{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PushT discretization verification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook inspects the discretized PushT datasets, confirms that every new artifact was generated, and compares\n",
        "them against the original continuous-action data. It is intended to be rerunnable whenever the discretization pipeline\n",
        "is updated or a new dataset copy is produced.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "de4dafb3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source dataset       : /Users/julianquast/Documents/Documents - pythonPedro (Mac)/Bachelor Thesis/Datasets/pusht_noise\n",
            "Discretized dataset  : /Users/julianquast/Documents/Documents - pythonPedro (Mac)/Bachelor Thesis/Datasets/pusht_noise_discretized\n",
            "Subset roots         : {'1k': PosixPath('/Users/julianquast/Documents/Documents - pythonPedro (Mac)/Bachelor Thesis/Datasets/pusht_noise_discretized_1k'), '10k': PosixPath('/Users/julianquast/Documents/Documents - pythonPedro (Mac)/Bachelor Thesis/Datasets/pusht_noise_discretized_10k')}\n",
            "Variants             : ('kmeans_k9', 'kmeans_k15', 'fixed_compass_r24')\n"
          ]
        }
      ],
      "source": [
        "# === Imports & configuration ===\n",
        "import os, gc, math, pickle\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "SPLITS = (\"train\", \"val\")\n",
        "ACTIONS_FNAME = \"rel_actions.pth\"\n",
        "ABS_ACTIONS_FNAME = \"abs_actions.pth\"\n",
        "VELOCITIES_FNAME = \"velocities.pth\"\n",
        "STATES_FNAME = \"states.pth\"\n",
        "STATE_CONSTANT_FNAME = \"states_constant.pth\"\n",
        "LENGTHS_FNAME = \"seq_lengths.pkl\"\n",
        "TOKENS_FNAME = \"tokens.pth\"\n",
        "VARIANTS = (\"kmeans_k9\", \"kmeans_k15\", \"fixed_compass_r24\")\n",
        "SUBSET_SPECS = {\"1k\": 1_000, \"10k\": 10_000}\n",
        "\n",
        "def _resolve_dataset_dir(path_str: str, default_leaf: str = \"pusht_noise\") -> Path:\n",
        "    base = Path(path_str).expanduser()\n",
        "    if (base / \"train\").exists() and (base / \"val\").exists():\n",
        "        return base\n",
        "    candidate = base / default_leaf\n",
        "    if (candidate / \"train\").exists() and (candidate / \"val\").exists():\n",
        "        return candidate\n",
        "    raise FileNotFoundError(f\"Could not resolve dataset directory from {path_str!r}. Provide SRC_DATASET_DIR or DATASET_DIR.\")\n",
        "\n",
        "src_root_env = os.getenv(\"SRC_DATASET_DIR\", os.getenv(\"DATASET_DIR\", \"data\"))\n",
        "SRC_DATASET_DIR = _resolve_dataset_dir(src_root_env)\n",
        "dst_root_env = os.getenv(\"DST_DATASET_DIR\", str(SRC_DATASET_DIR.parent / \"pusht_noise_discretized\"))\n",
        "DST_DATASET_DIR = Path(dst_root_env).expanduser()\n",
        "\n",
        "subset_roots = {name: DST_DATASET_DIR.parent / f\"{DST_DATASET_DIR.name}_{name}\" for name in SUBSET_SPECS}\n",
        "\n",
        "print(f\"Source dataset       : {SRC_DATASET_DIR}\")\n",
        "print(f\"Discretized dataset  : {DST_DATASET_DIR}\")\n",
        "print(f\"Subset roots         : {subset_roots}\")\n",
        "print(f\"Variants             : {VARIANTS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2632fab2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Helper utilities ===\n",
        "def load_lengths(split_dir: Path) -> np.ndarray:\n",
        "    with open(split_dir / LENGTHS_FNAME, \"rb\") as f:\n",
        "        lengths = pickle.load(f)\n",
        "    return np.asarray(lengths, dtype=np.int64)\n",
        "\n",
        "def list_split_files(split_dir: Path) -> List[str]:\n",
        "    return sorted(p.name for p in split_dir.iterdir() if p.is_file())\n",
        "\n",
        "def describe_tensor(tensor: torch.Tensor) -> Dict[str, object]:\n",
        "    return {\"shape\": tuple(int(x) for x in tensor.shape),\n",
        "            \"dtype\": str(tensor.dtype),\n",
        "            \"min\": float(tensor.min().item()),\n",
        "            \"max\": float(tensor.max().item())}\n",
        "\n",
        "def gather_episode_meta(dataset_dir: Path) -> Dict[str, Dict[str, object]]:\n",
        "    meta: Dict[str, Dict[str, object]] = {}\n",
        "    for split in SPLITS:\n",
        "        split_dir = dataset_dir / split\n",
        "        lengths = load_lengths(split_dir)\n",
        "        if lengths.size == 0:\n",
        "            min_len = max_len = total_steps = 0\n",
        "        else:\n",
        "            min_len = int(lengths.min())\n",
        "            max_len = int(lengths.max())\n",
        "            total_steps = int(lengths.sum())\n",
        "        meta[split] = {\n",
        "            \"num_episodes\": int(len(lengths)),\n",
        "            \"num_steps\": total_steps,\n",
        "            \"min_len\": min_len,\n",
        "            \"max_len\": max_len,\n",
        "            \"lengths\": lengths,\n",
        "        }\n",
        "    return meta\n",
        "\n",
        "def allocate_subset_counts(total_target: int, split_meta: Dict[str, Dict[str, int]]) -> Dict[str, int]:\n",
        "    total_available = sum(meta[\"num_episodes\"] for meta in split_meta.values())\n",
        "    if total_target >= total_available:\n",
        "        return {split: split_meta[split][\"num_episodes\"] for split in SPLITS}\n",
        "    counts: Dict[str, int] = {}\n",
        "    remaining = total_target\n",
        "    for idx, split in enumerate(SPLITS):\n",
        "        meta = split_meta[split]\n",
        "        if idx == len(SPLITS) - 1:\n",
        "            take = remaining\n",
        "        else:\n",
        "            prop = meta[\"num_episodes\"] / total_available\n",
        "            take = min(meta[\"num_episodes\"], int(round(total_target * prop)))\n",
        "        counts[split] = max(0, min(meta[\"num_episodes\"], take))\n",
        "        remaining -= counts[split]\n",
        "    while remaining > 0:\n",
        "        for split in SPLITS:\n",
        "            if counts[split] < split_meta[split][\"num_episodes\"]:\n",
        "                counts[split] += 1\n",
        "                remaining -= 1\n",
        "                if remaining == 0:\n",
        "                    break\n",
        "    return counts\n",
        "\n",
        "EPISODE_ALIGNED_FILES = [\n",
        "    ABS_ACTIONS_FNAME,\n",
        "    VELOCITIES_FNAME,\n",
        "    ACTIONS_FNAME,\n",
        "    STATES_FNAME,\n",
        "    STATE_CONSTANT_FNAME,\n",
        "    TOKENS_FNAME,\n",
        "] + [f\"rel_actions_discretized_{variant}.pth\" for variant in VARIANTS]\n",
        "\n",
        "def _episode_count_from_obj(obj) -> int | None:\n",
        "    if torch.is_tensor(obj):\n",
        "        return int(obj.shape[0])\n",
        "    if isinstance(obj, (list, tuple)):\n",
        "        return len(obj)\n",
        "    return None\n",
        "\n",
        "def verify_episode_aligned_files(split_dir: Path, expected_count: int):\n",
        "    for fname in EPISODE_ALIGNED_FILES:\n",
        "        path = split_dir / fname\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"Missing {path}\")\n",
        "        obj = torch.load(path, map_location=\"cpu\")\n",
        "        count = _episode_count_from_obj(obj)\n",
        "        if count is None:\n",
        "            continue\n",
        "        if count != expected_count:\n",
        "            raise ValueError(f\"{path} has {count} episodes, expected {expected_count}\")\n",
        "\n",
        "def count_observation_files(obs_dir: Path) -> int:\n",
        "    if not obs_dir.exists():\n",
        "        return 0\n",
        "    return sum(1 for _ in obs_dir.glob(\"episode_*\"))\n",
        "\n\ndef flatten_valid_tensor(tensor: torch.Tensor, lengths: np.ndarray) -> torch.Tensor:\n    lengths = np.asarray(lengths, dtype=np.int64)\n    if tensor.shape[0] != len(lengths):\n        raise ValueError(f\"Mismatch between tensor batch {tensor.shape[0]} and lengths {len(lengths)}\")\n    chunks: List[torch.Tensor] = []\n    for epi, L in enumerate(lengths):\n        L_int = int(L)\n        if L_int > 0:\n            chunks.append(tensor[epi, :L_int])\n    if not chunks:\n        return torch.empty((0, tensor.shape[-1]), dtype=tensor.dtype)\n    return torch.cat(chunks, dim=0)\n\n\ndef sample_valid_positions(lengths: np.ndarray, num_samples: int, rng=None) -> List[Tuple[int, int]]:\n    lengths = np.asarray(lengths, dtype=np.int64)\n    total_steps = int(lengths.sum())\n    if num_samples <= 0 or total_steps == 0:\n        return []\n    if rng is None:\n        rng = np.random.default_rng()\n    count = min(num_samples, total_steps)\n    flat_indices = np.asarray(rng.choice(total_steps, size=count, replace=False), dtype=np.int64)\n    flat_indices.sort()\n    cumulative = lengths.cumsum()\n    samples: List[Tuple[int, int]] = []\n    for idx in flat_indices:\n        epi = int(np.searchsorted(cumulative, idx, side='right'))\n        prev = int(cumulative[epi - 1]) if epi > 0 else 0\n        step = int(idx - prev)\n        samples.append((epi, step))\n    return samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset normalization stats\n",
        "Compute per-dimension mean and standard deviation for a chosen dataset to help with normalization. Update the dataset root or splits as needed before running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Dataset normalization stats ===\n",
        "NORMALIZATION_DATASET_DIR = SRC_DATASET_DIR  # change to DST_DATASET_DIR or a subset root if desired\n",
        "NORMALIZATION_SPLITS = (\"train\",)  # typically normalization is computed on the train split\n",
        "TARGET_FILES = (STATES_FNAME, VELOCITIES_FNAME, ACTIONS_FNAME, ABS_ACTIONS_FNAME)\n",
        "\n",
        "def compute_mean_std(split_dir: Path, lengths: np.ndarray, fname: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    path = split_dir / fname\n",
        "    tensor = torch.load(path, map_location=\"cpu\")\n",
        "    flat = flatten_valid_tensor(tensor, lengths)\n",
        "    if flat.numel() == 0:\n",
        "        raise ValueError(f\"No valid samples found in {path}\")\n",
        "    mean = flat.mean(dim=0)\n",
        "    std = flat.std(dim=0, unbiased=False)\n",
        "    return mean, std\n",
        "\n",
        "print(\"== Normalization statistics ==\")\n",
        "for split in NORMALIZATION_SPLITS:\n",
        "    split_dir = NORMALIZATION_DATASET_DIR / split\n",
        "    lengths = load_lengths(split_dir)\n",
        "    if lengths.size == 0:\n",
        "        print(f\"[skip] No episodes found for {split_dir}\")\n",
        "        continue\n",
        "    for fname in TARGET_FILES:\n",
        "        path = split_dir / fname\n",
        "        if not path.exists():\n",
        "            print(f\"[skip] Missing {path}\")\n",
        "            continue\n",
        "        mean, std = compute_mean_std(split_dir, lengths, fname)\n",
        "        print(f\"[{split}] {fname}: mean shape={tuple(mean.shape)}, std shape={tuple(std.shape)}\")\n",
        "        print(f\"    mean={mean.tolist()}\")\n",
        "        print(f\"    std ={std.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c4c34b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === File inventory comparison ===\n",
        "print(\"== File inventory comparison between source and discretized roots ==\")\n",
        "for split in SPLITS:\n",
        "    src_files = set(list_split_files(SRC_DATASET_DIR / split))\n",
        "    dst_files = set(list_split_files(DST_DATASET_DIR / split))\n",
        "    missing = sorted(src_files - dst_files)\n",
        "    extras = sorted(dst_files - src_files)\n",
        "    print(f\"[{split}] missing in discretized: {missing if missing else 'None'}\")\n",
        "    print(f\"[{split}] new files          : {extras if extras else 'None'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1a308014",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[train] episode counts\n",
            "source episodes     : 18685\n",
            "discretized episodes: 18685\n",
            "rel_actions identical: True (max diff=0.000e+00)\n",
            "  kmeans_k9: mean|\u0394|=3.7636, max|\u0394|=153.7511, unique centroids=10\n",
            "  kmeans_k15: mean|\u0394|=3.0071, max|\u0394|=144.6930, unique centroids=16\n",
            "  fixed_compass_r24: mean|\u0394|=4.6653, max|\u0394|=180.2670, unique centroids=5\n",
            "states_constant zeros check: max|value|=0.000e+00\n",
            "\n",
            "[val] episode counts\n",
            "source episodes     : 21\n",
            "discretized episodes: 21\n",
            "rel_actions identical: True (max diff=0.000e+00)\n",
            "  kmeans_k9: mean|\u0394|=3.5948, max|\u0394|=71.3540, unique centroids=10\n",
            "  kmeans_k15: mean|\u0394|=2.8930, max|\u0394|=66.2523, unique centroids=16\n",
            "  fixed_compass_r24: mean|\u0394|=4.4253, max|\u0394|=89.8806, unique centroids=5\n",
            "states_constant zeros check: max|value|=0.000e+00\n"
          ]
        }
      ],
      "source": [
        "# === Detailed comparison: source vs discretized base dataset ===\n",
        "variant_stats: List[Dict[str, object]] = []\n",
        "source_meta = gather_episode_meta(SRC_DATASET_DIR)\n",
        "discretized_meta = gather_episode_meta(DST_DATASET_DIR)\n",
        "for split in SPLITS:\n",
        "    print(f\"\\n[{split}] episode counts\")\n",
        "    print(f\"source episodes     : {source_meta[split]['num_episodes']}\")\n",
        "    print(f\"discretized episodes: {discretized_meta[split]['num_episodes']}\")\n",
        "    if source_meta[split]['num_episodes'] != discretized_meta[split]['num_episodes']:\n",
        "        raise ValueError(f\"Episode counts disagree for {split}\")\n",
        "    src_lengths = source_meta[split]['lengths']\n",
        "    dst_lengths = discretized_meta[split]['lengths']\n",
        "    if not np.array_equal(src_lengths, dst_lengths):\n",
        "        raise ValueError(f\"Sequence lengths diverged for {split}\")\n",
        "    split_dir_src = SRC_DATASET_DIR / split\n",
        "    split_dir_dst = DST_DATASET_DIR / split\n",
        "    # Compare rel_actions tensors\n",
        "    src_actions = torch.load(split_dir_src / ACTIONS_FNAME, map_location=\"cpu\")\n",
        "    dst_actions = torch.load(split_dir_dst / ACTIONS_FNAME, map_location=\"cpu\")\n",
        "    if src_actions.shape != dst_actions.shape:\n",
        "        raise ValueError(f\"rel_actions shape mismatch for {split}\")\n",
        "    if src_actions.dtype != dst_actions.dtype:\n",
        "        raise ValueError(f\"rel_actions dtype mismatch for {split}\")\n",
        "    actions_equal = bool(torch.equal(src_actions, dst_actions))\n",
        "    max_abs_diff = float((dst_actions - src_actions).abs().max().item())\n",
        "    print(f\"rel_actions identical: {actions_equal} (max diff={max_abs_diff:.3e})\")\n",
        "    # Compare auxiliary continuous files\n",
        "    for fname in (ABS_ACTIONS_FNAME, VELOCITIES_FNAME):\n",
        "        src_tensor = torch.load(split_dir_src / fname, map_location=\"cpu\")\n",
        "        dst_tensor = torch.load(split_dir_dst / fname, map_location=\"cpu\")\n",
        "        if not torch.equal(src_tensor, dst_tensor):\n",
        "            raise ValueError(f\"{fname} mismatches for {split}\")\n",
        "    tokens_src = torch.load(split_dir_src / TOKENS_FNAME, map_location=\"cpu\")\n",
        "    tokens_dst = torch.load(split_dir_dst / TOKENS_FNAME, map_location=\"cpu\")\n",
        "    if len(tokens_src) != len(tokens_dst):\n",
        "        raise ValueError(f\"tokens length mismatch for {split}\")\n",
        "    # Validate discretized variants\n",
        "    for variant in VARIANTS:\n",
        "        var_path = split_dir_dst / f\"rel_actions_discretized_{variant}.pth\"\n",
        "        if not var_path.exists():\n",
        "            raise FileNotFoundError(f\"Missing {var_path}\")\n",
        "        var_tensor = torch.load(var_path, map_location=\"cpu\")\n",
        "        if var_tensor.shape != src_actions.shape:\n",
        "            raise ValueError(f\"Variant {variant} shape mismatch for {split}\")\n",
        "        if var_tensor.dtype != src_actions.dtype:\n",
        "            raise ValueError(f\"Variant {variant} dtype mismatch for {split}\")\n",
        "        delta = var_tensor - src_actions\n",
        "        mean_abs = float(delta.abs().mean().item())\n",
        "        max_abs = float(delta.abs().max().item())\n",
        "        flat = var_tensor.reshape(-1, var_tensor.shape[-1])\n",
        "        unique_centroids = int(torch.unique(flat, dim=0).shape[0])\n",
        "        variant_stats.append({\"split\": split, \"variant\": variant, \"mean_abs_diff\": mean_abs, \"max_abs_diff\": max_abs, \"unique_centroids\": unique_centroids})\n",
        "        print(f\"  {variant}: mean|\u0394|={mean_abs:.4f}, max|\u0394|={max_abs:.4f}, unique centroids={unique_centroids}\")\n",
        "        del var_tensor, delta, flat\n",
        "    states = torch.load(split_dir_dst / STATES_FNAME, map_location=\"cpu\")\n",
        "    states_const = torch.load(split_dir_dst / STATE_CONSTANT_FNAME, map_location=\"cpu\")\n",
        "    if states_const.shape != states.shape:\n",
        "        raise ValueError(f\"states_constant shape mismatch for {split}\")\n",
        "    zero_max = float(states_const.abs().max().item())\n",
        "    print(f\"states_constant zeros check: max|value|={zero_max:.3e}\")\n",
        "    del src_actions, dst_actions, states, states_const, tokens_src, tokens_dst\n",
        "    gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ad7ac92f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== Checking subset 1k (1000 requested trajectories) ==\n",
            "expected total: 1000, actual total: 1000\n",
            "[1k/train] episodes: 999 (expected 999), steps: 125676\n",
            "    lengths range: [49, 246]\n",
            "[1k/val] episodes: 1 (expected 1), steps: 114\n",
            "    lengths range: [114, 114]\n",
            "\n",
            "== Checking subset 10k (10000 requested trajectories) ==\n",
            "expected total: 10000, actual total: 10000\n",
            "[10k/train] episodes: 9989 (expected 9989), steps: 1243713\n",
            "    lengths range: [49, 246]\n",
            "[10k/val] episodes: 11 (expected 11), steps: 1317\n",
            "    lengths range: [76, 167]\n"
          ]
        }
      ],
      "source": [
        "# === Subset dataset verification (1k / 10k trajectories) ===\n",
        "base_counts_for_alloc = {split: {\"num_episodes\": discretized_meta[split][\"num_episodes\"]} for split in SPLITS}\n",
        "for subset_name, target_total in SUBSET_SPECS.items():\n",
        "    subset_root = subset_roots[subset_name]\n",
        "    print(f\"\\n== Checking subset {subset_name} ({target_total} requested trajectories) ==\")\n",
        "    if not subset_root.exists():\n",
        "        print(f\"[skip] {subset_root} does not exist\")\n",
        "        continue\n",
        "    subset_meta = gather_episode_meta(subset_root)\n",
        "    expected_counts = allocate_subset_counts(target_total, base_counts_for_alloc)\n",
        "    total_expected = sum(expected_counts.values())\n",
        "    total_actual = sum(meta[\"num_episodes\"] for meta in subset_meta.values())\n",
        "    print(f\"expected total: {total_expected}, actual total: {total_actual}\")\n",
        "    if total_actual != total_expected:\n",
        "        raise ValueError(f\"Subset {subset_name} has {total_actual} episodes, expected {total_expected}\")\n",
        "    for split in SPLITS:\n",
        "        expected = expected_counts.get(split, 0)\n",
        "        actual = subset_meta[split][\"num_episodes\"]\n",
        "        print(f\"[{subset_name}/{split}] episodes: {actual} (expected {expected}), steps: {subset_meta[split]['num_steps']}\")\n",
        "        if actual != expected:\n",
        "            raise ValueError(f\"Subset {subset_name} split {split} mismatch: {actual} vs {expected}\")\n",
        "        verify_episode_aligned_files(subset_root / split, actual)\n",
        "        lengths = subset_meta[split][\"lengths\"]\n",
        "        if lengths.size:\n",
        "            print(f\"    lengths range: [{lengths.min()}, {lengths.max()}]\")\n",
        "        obs_count = count_observation_files((subset_root / split) / \"obses\")\n",
        "        if obs_count and obs_count != actual:\n",
        "            raise ValueError(f\"Observation file count mismatch for {subset_name}/{split}: {obs_count} vs {actual}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e49749c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split   Variant                  mean|\u0394|      max|\u0394|   Centroids\n",
            "----------------------------------------------------------------\n",
            "train   kmeans_k9               3.763560  153.751147          10\n",
            "train   kmeans_k15              3.007122  144.692996          16\n",
            "train   fixed_compass_r24       4.665251  180.266978           5\n",
            "val     kmeans_k9               3.594826   71.354050          10\n",
            "val     kmeans_k15              2.893014   66.252327          16\n",
            "val     fixed_compass_r24       4.425332   89.880615           5\n"
          ]
        }
      ],
      "source": [
        "# === Aggregate discretization stats ===\n",
        "if not variant_stats:\n",
        "    print(\"No variant statistics collected. Run the previous cell first.\")\n",
        "else:\n",
        "    header = f\"{'Split':<8}{'Variant':<20}{'mean|\u0394|':>12}{'max|\u0394|':>12}{'Centroids':>12}\"\n",
        "    print(header)\n",
        "    print('-' * len(header))\n",
        "    for entry in variant_stats:\n",
        "        print(f\"{entry['split']:<8}{entry['variant']:<20}{entry['mean_abs_diff']:>12.6f}{entry['max_abs_diff']:>12.6f}{entry['unique_centroids']:>12}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Codebook inspection ===\n",
        "CODEBOOK_DIR = DST_DATASET_DIR / \"codebooks\"\n",
        "centroid_cache: Dict[str, torch.Tensor] = {}\n",
        "print(\"== Loading discretization centroids ==\")\n",
        "for variant in VARIANTS:\n",
        "    npy_path = CODEBOOK_DIR / f\"{variant}_centroids.npy\"\n",
        "    pt_path = CODEBOOK_DIR / f\"{variant}_centroids.pt\"\n",
        "    if npy_path.exists():\n",
        "        centers = np.load(npy_path)\n",
        "    elif pt_path.exists():\n",
        "        centers = torch.load(pt_path, map_location=\"cpu\")\n",
        "        if torch.is_tensor(centers):\n",
        "            centers = centers.cpu().numpy()\n",
        "    else:\n",
        "        print(f\"[warn] Missing centroids for {variant} in {CODEBOOK_DIR}\")\n",
        "        continue\n",
        "    centers_tensor = torch.as_tensor(centers, dtype=torch.float32)\n",
        "    centroid_cache[variant] = centers_tensor\n",
        "    print(f\"{variant}: shape={tuple(centers_tensor.shape)}\")\n",
        "    print(centers_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Unique centroid diagnostics (valid vs padded) ===\n",
        "print(\"== Unique centroid counts including/excluding padding ==\")\n",
        "for split in SPLITS:\n",
        "    lengths = load_lengths(DST_DATASET_DIR / split)\n",
        "    for variant in VARIANTS:\n",
        "        var_path = DST_DATASET_DIR / split / f\"rel_actions_discretized_{variant}.pth\"\n",
        "        if not var_path.exists():\n",
        "            print(f\"[skip] Missing {var_path}\")\n",
        "            continue\n",
        "        disc_tensor = torch.load(var_path, map_location=\"cpu\")\n",
        "        flat_all = disc_tensor.reshape(-1, disc_tensor.shape[-1])\n",
        "        unique_all = torch.unique(flat_all, dim=0)\n",
        "        flat_valid = flatten_valid_tensor(disc_tensor, lengths)\n",
        "        unique_valid = torch.unique(flat_valid, dim=0) if flat_valid.numel() else torch.empty((0, disc_tensor.shape[-1]), dtype=disc_tensor.dtype)\n",
        "        padding_vecs = []\n",
        "        if unique_valid.shape[0] < unique_all.shape[0]:\n",
        "            if unique_valid.numel():\n",
        "                dists = torch.cdist(unique_all, unique_valid)\n",
        "                unmatched = dists.min(dim=1).values > 1e-6\n",
        "                padding_vecs = unique_all[unmatched].tolist()\n",
        "            else:\n",
        "                padding_vecs = unique_all.tolist()\n",
        "        padding_str = f\", padding vectors={padding_vecs}\" if padding_vecs else \"\"\n",
        "        print(f\"[{split}/{variant}] unique(all)={unique_all.shape[0]}, unique(valid)={unique_valid.shape[0]}{padding_str}\")\n",
        "        del disc_tensor, flat_all, unique_all, flat_valid, unique_valid\n",
        "    gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Spot-check discretized samples ===\n",
        "SAMPLES_PER_SPLIT: Dict[str, List[Tuple[int, int]]] = {}\n",
        "rng = np.random.default_rng(0)\n",
        "NUM_SAMPLES = 5\n",
        "for split in SPLITS:\n",
        "    lengths = load_lengths(SRC_DATASET_DIR / split)\n",
        "    SAMPLES_PER_SPLIT[split] = sample_valid_positions(lengths, NUM_SAMPLES, rng=rng)\n",
        "    print(f\"Sampled positions for {split}: {SAMPLES_PER_SPLIT[split]}\")\n",
        "\n",
        "print(\"\\n== Discretization spot-checks ==\")\n",
        "for split in SPLITS:\n",
        "    src_actions = torch.load(SRC_DATASET_DIR / split / ACTIONS_FNAME, map_location=\"cpu\")\n",
        "    lengths = load_lengths(SRC_DATASET_DIR / split)\n",
        "    for variant in VARIANTS:\n",
        "        var_path = DST_DATASET_DIR / split / f\"rel_actions_discretized_{variant}.pth\"\n",
        "        if not var_path.exists():\n",
        "            print(f\"[skip] Missing {var_path}\")\n",
        "            continue\n",
        "        disc_tensor = torch.load(var_path, map_location=\"cpu\")\n",
        "        centroids = centroid_cache.get(variant)\n",
        "        if centroids is None:\n",
        "            print(f\"[skip] No centroids loaded for {variant}\")\n",
        "            continue\n",
        "        print(f\"\\n[{split}/{variant}] Checking {len(SAMPLES_PER_SPLIT[split])} samples\")\n",
        "        for epi, step in SAMPLES_PER_SPLIT[split]:\n",
        "            if step >= lengths[epi]:\n",
        "                print(f\"  [skip] episode {epi} shorter than step {step}\")\n",
        "                continue\n",
        "            orig = src_actions[epi, step]\n",
        "            disc = disc_tensor[epi, step]\n",
        "            dists = torch.norm(centroids - disc, dim=1)\n",
        "            centroid_idx = int(torch.argmin(dists).item())\n",
        "            centroid = centroids[centroid_idx]\n",
        "            disc_centroid_max = float((disc - centroid).abs().max().item())\n",
        "            orig_centroid_dist = float(torch.norm(orig - centroid).item())\n",
        "            orig_disc_dist = float(torch.norm(orig - disc).item())\n",
        "            print(f\"  epi={epi:05d}, step={step:03d}, centroid={centroid_idx:02d}, |orig-centroid|={orig_centroid_dist:.4f}, |orig-disc|={orig_disc_dist:.4f}, max|disc-centroid|={disc_centroid_max:.2e}\")\n",
        "            print(f\"    original    : {orig.tolist()}\")\n",
        "            print(f\"    centroid    : {centroid.tolist()}\")\n",
        "            print(f\"    discretized : {disc.tolist()}\")\n",
        "        del disc_tensor\n",
        "    del src_actions\n",
        "    gc.collect()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dino_wm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
