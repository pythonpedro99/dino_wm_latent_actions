{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f8ee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dir  : /Users/julianquast/Downloads/pusht_noise\n",
      "Subset specs : {'1k': 1000, '10k': 10000}\n",
      "Random seed  : 0\n",
      "Original train episodes: 18,685\n",
      "\n",
      "== Building subset 1k (1,000 trajectories) ==\n",
      "[1k] new train=900, new val=100 (both from original train)\n",
      "Subset written to /Users/julianquast/Downloads/pusht_noise_1k\n",
      "\n",
      "== Building subset 10k (10,000 trajectories) ==\n",
      "[10k] new train=9,000, new val=1,000 (both from original train)\n",
      "Subset written to /Users/julianquast/Downloads/pusht_noise_10k\n",
      "\n",
      "Done. Subset roots:\n",
      "  1k: /Users/julianquast/Downloads/pusht_noise_1k\n",
      "  10k: /Users/julianquast/Downloads/pusht_noise_10k\n"
     ]
    }
   ],
   "source": [
    "# === SAFE build: uniform-random 1k / 10k subsets (no discretization), train/val both from ORIGINAL train ===\n",
    "# Output:\n",
    "#   <DATASET_DIR>_1k/{train,val}/...\n",
    "#   <DATASET_DIR>_10k/{train,val}/...\n",
    "\n",
    "import gc, json, pickle, shutil, re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "LENGTHS_FNAME = \"seq_lengths.pkl\"\n",
    "\n",
    "SUBSET_SPECS = {\n",
    "    \"1k\": 1_000,\n",
    "    \"10k\": 10_000,\n",
    "}\n",
    "\n",
    "DATASET_DIR = Path(\"/Users/julianquast/Downloads/pusht_noise\").expanduser()\n",
    "\n",
    "try:\n",
    "    SUBSET_SEED\n",
    "except NameError:\n",
    "    SUBSET_SEED = 0\n",
    "\n",
    "RANDOM_SEED = int(SUBSET_SEED)\n",
    "\n",
    "# set True if you want to rebuild an existing <DATASET_DIR>_10k etc.\n",
    "OVERWRITE = False\n",
    "\n",
    "assert (DATASET_DIR / \"train\").is_dir(), f\"Missing train/: {DATASET_DIR / 'train'}\"\n",
    "assert (DATASET_DIR / \"val\").is_dir(), f\"Missing val/: {DATASET_DIR / 'val'}\"\n",
    "\n",
    "print(f\"Dataset dir  : {DATASET_DIR}\")\n",
    "print(f\"Subset specs : {SUBSET_SPECS}\")\n",
    "print(f\"Random seed  : {RANDOM_SEED}\")\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def load_lengths_only(split_dir: Path) -> np.ndarray:\n",
    "    p = split_dir / LENGTHS_FNAME\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing {LENGTHS_FNAME} in {split_dir}\")\n",
    "    with open(p, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    arr = data.astype(np.int64, copy=False) if isinstance(data, np.ndarray) else np.asarray(data, dtype=np.int64)\n",
    "    if arr.ndim != 1:\n",
    "        arr = arr.reshape(-1)\n",
    "    return arr\n",
    "\n",
    "def _subset_obj(obj: Any, idxs: np.ndarray, orig_count: int) -> Tuple[Any, bool]:\n",
    "    \"\"\"Returns (subset_obj, changed_flag).\"\"\"\n",
    "    if torch.is_tensor(obj):\n",
    "        if obj.ndim >= 1 and obj.shape[0] == orig_count:\n",
    "            out = obj.index_select(0, torch.as_tensor(idxs, dtype=torch.long))\n",
    "            return out, True\n",
    "        return obj, False\n",
    "\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        if obj.ndim >= 1 and obj.shape[0] == orig_count:\n",
    "            return obj[idxs], True\n",
    "        return obj, False\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        if len(obj) == orig_count:\n",
    "            return [obj[int(i)] for i in idxs], True\n",
    "        return obj, False\n",
    "\n",
    "    if isinstance(obj, tuple):\n",
    "        if len(obj) == orig_count:\n",
    "            return tuple(obj[int(i)] for i in idxs), True\n",
    "        return obj, False\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        changed = False\n",
    "        out = {}\n",
    "        for k, v in obj.items():\n",
    "            vv, ch = _subset_obj(v, idxs, orig_count)\n",
    "            out[k] = vv\n",
    "            changed = changed or ch\n",
    "        return out, changed\n",
    "\n",
    "    return obj, False\n",
    "\n",
    "def torch_load_safe(path: Path) -> Any:\n",
    "    \"\"\"\n",
    "    Safe loader:\n",
    "      - tries mmap=True with *string filename* (required by torch internals)\n",
    "      - falls back to normal torch.load on any error\n",
    "    \"\"\"\n",
    "    # IMPORTANT: mmap=True requires a string path in some torch versions\n",
    "    path_str = str(path)\n",
    "    try:\n",
    "        return torch.load(path_str, map_location=\"cpu\", mmap=True)\n",
    "    except Exception:\n",
    "        return torch.load(path_str, map_location=\"cpu\")\n",
    "\n",
    "def subset_tensor_to_path(src_path: Path, dst_path: Path, idxs: np.ndarray, orig_count: int) -> bool:\n",
    "    data = torch_load_safe(src_path)\n",
    "    new_data, changed = _subset_obj(data, idxs, orig_count)\n",
    "    if changed:\n",
    "        dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(new_data, str(dst_path))\n",
    "    # free\n",
    "    del data, new_data\n",
    "    gc.collect()\n",
    "    return changed\n",
    "\n",
    "def subset_pickle_to_path(src_path: Path, dst_path: Path, idxs: np.ndarray, orig_count: int) -> bool:\n",
    "    with open(src_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    new_data, changed = _subset_obj(data, idxs, orig_count)\n",
    "    if changed:\n",
    "        dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(dst_path, \"wb\") as f:\n",
    "            pickle.dump(new_data, f)\n",
    "    del data, new_data\n",
    "    gc.collect()\n",
    "    return changed\n",
    "\n",
    "def build_episode_file_map(obs_dir: Path) -> Dict[int, Path]:\n",
    "    mapping: Dict[int, Path] = {}\n",
    "    if not obs_dir.exists():\n",
    "        return mapping\n",
    "    pattern = re.compile(r\"episode_(\\d+)\")\n",
    "    for path in obs_dir.glob(\"episode_*\"):\n",
    "        m = pattern.search(path.stem)\n",
    "        if m:\n",
    "            mapping[int(m.group(1))] = path\n",
    "    return mapping\n",
    "\n",
    "def copy_subset_observation_media(src_obs_dir: Path, dst_obs_dir: Path, idxs: np.ndarray):\n",
    "    \"\"\"Copy only selected episode_* files from src_obs_dir to dst_obs_dir, renaming to contiguous indices.\"\"\"\n",
    "    if not src_obs_dir.exists():\n",
    "        return\n",
    "    idxs = np.asarray(idxs, dtype=np.int64)\n",
    "    mapping = build_episode_file_map(src_obs_dir)\n",
    "\n",
    "    if dst_obs_dir.exists():\n",
    "        shutil.rmtree(dst_obs_dir)\n",
    "    dst_obs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pad = max(3, len(str(len(idxs) - 1 if len(idxs) else 0)))\n",
    "    for new_idx, old_idx in enumerate(idxs):\n",
    "        src_path = mapping.get(int(old_idx))\n",
    "        if src_path is None:\n",
    "            raise FileNotFoundError(f\"Missing obs file for episode {old_idx} in {src_obs_dir}\")\n",
    "        dst_path = dst_obs_dir / f\"episode_{new_idx:0{pad}d}{src_path.suffix}\"\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "def copy_non_split_extras(src_root: Path, dst_root: Path):\n",
    "    \"\"\"Copy everything at dataset root except train/ and val/ into dst_root.\"\"\"\n",
    "    dst_root.mkdir(parents=True, exist_ok=True)\n",
    "    for item in src_root.iterdir():\n",
    "        if item.name in (\"train\", \"val\"):\n",
    "            continue\n",
    "        dst_item = dst_root / item.name\n",
    "        if item.is_dir():\n",
    "            shutil.copytree(item, dst_item, dirs_exist_ok=True)\n",
    "        else:\n",
    "            dst_item.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(item, dst_item)\n",
    "\n",
    "def write_train_val_from_train_split(\n",
    "    src_train_dir: Path,\n",
    "    out_train_dir: Path,\n",
    "    out_val_dir: Path,\n",
    "    train_idxs: np.ndarray,\n",
    "    val_idxs: np.ndarray,\n",
    "    orig_count: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build out_train_dir and out_val_dir from src_train_dir.\n",
    "\n",
    "    For each item in src_train_dir:\n",
    "      - obses/ is subset-copied for each output\n",
    "      - *.pth/*.pt and *.pkl are subsetted if episode-aligned; otherwise copied as-is\n",
    "      - other files/dirs are copied as-is to both outputs\n",
    "    \"\"\"\n",
    "    out_train_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_val_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for item in sorted(src_train_dir.iterdir()):\n",
    "        if item.is_dir():\n",
    "            if item.name == \"obses\":\n",
    "                copy_subset_observation_media(item, out_train_dir / \"obses\", train_idxs)\n",
    "                copy_subset_observation_media(item, out_val_dir / \"obses\", val_idxs)\n",
    "            else:\n",
    "                shutil.copytree(item, out_train_dir / item.name, dirs_exist_ok=True)\n",
    "                shutil.copytree(item, out_val_dir / item.name, dirs_exist_ok=True)\n",
    "            gc.collect()\n",
    "            continue\n",
    "\n",
    "        suffix = item.suffix.lower()\n",
    "        dst_train = out_train_dir / item.name\n",
    "        dst_val   = out_val_dir / item.name\n",
    "\n",
    "        if suffix in (\".pth\", \".pt\"):\n",
    "            changed_train = subset_tensor_to_path(item, dst_train, train_idxs, orig_count)\n",
    "            changed_val   = subset_tensor_to_path(item, dst_val,   val_idxs,   orig_count)\n",
    "\n",
    "            if not changed_train:\n",
    "                dst_train.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy2(item, dst_train)\n",
    "            if not changed_val:\n",
    "                dst_val.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy2(item, dst_val)\n",
    "\n",
    "        elif suffix == \".pkl\":\n",
    "            changed_train = subset_pickle_to_path(item, dst_train, train_idxs, orig_count)\n",
    "            changed_val   = subset_pickle_to_path(item, dst_val,   val_idxs,   orig_count)\n",
    "\n",
    "            if not changed_train:\n",
    "                dst_train.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy2(item, dst_train)\n",
    "            if not changed_val:\n",
    "                dst_val.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy2(item, dst_val)\n",
    "\n",
    "        else:\n",
    "            dst_train.parent.mkdir(parents=True, exist_ok=True)\n",
    "            dst_val.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(item, dst_train)\n",
    "            shutil.copy2(item, dst_val)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "# -------------------- Episode counts --------------------\n",
    "src_train_dir = DATASET_DIR / \"train\"\n",
    "train_avail = int(len(load_lengths_only(src_train_dir)))\n",
    "print(f\"Original train episodes: {train_avail:,}\")\n",
    "assert train_avail > 0, \"No train episodes available.\"\n",
    "\n",
    "# -------------------- Build subsets --------------------\n",
    "subset_roots: Dict[str, Path] = {}\n",
    "\n",
    "for subset_name, target_total in SUBSET_SPECS.items():\n",
    "    if target_total > train_avail:\n",
    "        raise ValueError(f\"Requested {target_total} episodes but only {train_avail} train episodes available.\")\n",
    "\n",
    "    subset_root = DATASET_DIR.parent / f\"{DATASET_DIR.name}_{subset_name}\"\n",
    "    subset_roots[subset_name] = subset_root\n",
    "\n",
    "    if subset_root.exists():\n",
    "        if OVERWRITE:\n",
    "            print(f\"[overwrite] removing existing {subset_root}\")\n",
    "            shutil.rmtree(subset_root)\n",
    "        else:\n",
    "            print(f\"[skip] {subset_root} already exists\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n== Building subset {subset_name} ({target_total:,} trajectories) ==\")\n",
    "\n",
    "    # Copy extras (anything not train/val)\n",
    "    subset_root.mkdir(parents=True, exist_ok=True)\n",
    "    copy_non_split_extras(DATASET_DIR, subset_root)\n",
    "\n",
    "    # Uniform sample from ORIGINAL train, then disjoint 90/10 split\n",
    "    rng = np.random.default_rng(RANDOM_SEED + target_total)\n",
    "    sampled = rng.choice(train_avail, size=target_total, replace=False)\n",
    "    perm = rng.permutation(len(sampled))\n",
    "\n",
    "    train_target = int(round(target_total * 0.9))\n",
    "    val_target   = target_total - train_target\n",
    "\n",
    "    train_idxs = np.sort(sampled[perm[:train_target]])\n",
    "    val_idxs   = np.sort(sampled[perm[train_target:]])\n",
    "\n",
    "    assert len(train_idxs) == train_target and len(val_idxs) == val_target\n",
    "    assert len(np.intersect1d(train_idxs, val_idxs)) == 0\n",
    "\n",
    "    print(f\"[{subset_name}] new train={len(train_idxs):,}, new val={len(val_idxs):,} (both from original train)\")\n",
    "\n",
    "    out_train_dir = subset_root / \"train\"\n",
    "    out_val_dir   = subset_root / \"val\"\n",
    "\n",
    "    write_train_val_from_train_split(\n",
    "        src_train_dir=src_train_dir,\n",
    "        out_train_dir=out_train_dir,\n",
    "        out_val_dir=out_val_dir,\n",
    "        train_idxs=train_idxs,\n",
    "        val_idxs=val_idxs,\n",
    "        orig_count=train_avail,\n",
    "    )\n",
    "\n",
    "    info = {\n",
    "        \"source_dataset\": str(DATASET_DIR),\n",
    "        \"subset_name\": subset_name,\n",
    "        \"target_total\": int(target_total),\n",
    "        \"train_episodes\": int(len(train_idxs)),\n",
    "        \"val_episodes\": int(len(val_idxs)),\n",
    "        \"orig_train_episodes\": int(train_avail),\n",
    "        \"seed\": int(RANDOM_SEED),\n",
    "        \"sampling\": \"uniform sample target_total from original train (no replacement); permute and split 90/10 (disjoint).\",\n",
    "        \"overwrite\": bool(OVERWRITE),\n",
    "    }\n",
    "    with open(subset_root / \"subset_info.json\", \"w\") as f:\n",
    "        json.dump(info, f, indent=2)\n",
    "\n",
    "    gc.collect()\n",
    "    print(f\"Subset written to {subset_root}\")\n",
    "\n",
    "print(\"\\nDone. Subset roots:\")\n",
    "for k, v in subset_roots.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f123f21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Checking subset: /Users/julianquast/Downloads/pusht_noise_1k ===\n",
      "  split: train\n",
      "    episodes (seq_lengths): 900 | minT=49 maxT=246 meanT=125.83\n",
      "    obses/: 900 episode_* files\n",
      "    - rel_actions.pth: OK shape=(900, 246, 2)\n",
      "    - states.pth: OK shape=(900, 246, 5)\n",
      "    - states_constant.pth: (missing) [SKIP]\n",
      "    other tensor files (top-level): 3\n",
      "    - abs_actions.pth: OK shape=(900, 246, 2)\n",
      "    - tokens.pth: type=list [INFO]\n",
      "    - velocities.pth: OK shape=(900, 246, 2)\n",
      "    RESULT: OK\n",
      "\n",
      "  split: val\n",
      "    episodes (seq_lengths): 100 | minT=49 maxT=229 meanT=126.56\n",
      "    obses/: 100 episode_* files\n",
      "    - rel_actions.pth: OK shape=(100, 246, 2)\n",
      "    - states.pth: OK shape=(100, 246, 5)\n",
      "    - states_constant.pth: (missing) [SKIP]\n",
      "    other tensor files (top-level): 3\n",
      "    - abs_actions.pth: OK shape=(100, 246, 2)\n",
      "    - tokens.pth: type=list [INFO]\n",
      "    - velocities.pth: OK shape=(100, 246, 2)\n",
      "    RESULT: OK\n",
      "\n",
      "\n",
      "=== Checking subset: /Users/julianquast/Downloads/pusht_noise_10k ===\n",
      "  split: train\n",
      "    episodes (seq_lengths): 9,000 | minT=49 maxT=246 meanT=124.68\n",
      "    obses/: 9,000 episode_* files\n",
      "    - rel_actions.pth: OK shape=(9000, 246, 2)\n",
      "    - states.pth: OK shape=(9000, 246, 5)\n",
      "    - states_constant.pth: (missing) [SKIP]\n",
      "    other tensor files (top-level): 3\n",
      "    - abs_actions.pth: OK shape=(9000, 246, 2)\n",
      "    - tokens.pth: type=list [INFO]\n",
      "    - velocities.pth: OK shape=(9000, 246, 2)\n",
      "    RESULT: OK\n",
      "\n",
      "  split: val\n",
      "    episodes (seq_lengths): 1,000 | minT=49 maxT=246 meanT=123.03\n",
      "    obses/: 1,000 episode_* files\n",
      "    - rel_actions.pth: OK shape=(1000, 246, 2)\n",
      "    - states.pth: OK shape=(1000, 246, 5)\n",
      "    - states_constant.pth: (missing) [SKIP]\n",
      "    other tensor files (top-level): 3\n",
      "    - abs_actions.pth: OK shape=(1000, 246, 2)\n",
      "    - tokens.pth: type=list [INFO]\n",
      "    - velocities.pth: OK shape=(1000, 246, 2)\n",
      "    RESULT: OK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Sanity-check subsets: seq_lengths, obses, and episode-aligned action/state tensors ===\n",
    "# Checks:\n",
    "#  - seq_lengths.pkl exists, sane, and defines N episodes\n",
    "#  - obses/episode_* count == N (if obses exists) and indices are contiguous\n",
    "#  - rel_actions.pth (if present) has first dim == N\n",
    "#  - states.pth / states_constant.pth (if present) has first dim == N\n",
    "#  - any other *.pth/*.pt tensors at top-level with first dim == N are reported\n",
    "\n",
    "import pickle, re, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "LENGTHS_FNAME = \"seq_lengths.pkl\"\n",
    "ACTIONS_FNAME = \"rel_actions.pth\"\n",
    "STATES_FNAME = \"states.pth\"\n",
    "STATES_CONST_FNAME = \"states_constant.pth\"\n",
    "\n",
    "DATASET_DIR = Path(\"/Users/julianquast/Downloads/pusht_noise\").expanduser()\n",
    "SUBSET_DIRS = [\n",
    "    DATASET_DIR.parent / f\"{DATASET_DIR.name}_1k\",\n",
    "    DATASET_DIR.parent / f\"{DATASET_DIR.name}_10k\",\n",
    "]\n",
    "\n",
    "def load_lengths(split_dir: Path) -> np.ndarray:\n",
    "    p = split_dir / LENGTHS_FNAME\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing {p}\")\n",
    "    with open(p, \"rb\") as f:\n",
    "        x = pickle.load(f)\n",
    "    x = x.astype(np.int64, copy=False) if isinstance(x, np.ndarray) else np.asarray(x, dtype=np.int64)\n",
    "    return x.reshape(-1)\n",
    "\n",
    "def count_obs_eps(obs_dir: Path) -> int:\n",
    "    if not obs_dir.exists():\n",
    "        return 0\n",
    "    pat = re.compile(r\"episode_(\\d+)\")\n",
    "    ids = []\n",
    "    for p in obs_dir.glob(\"episode_*\"):\n",
    "        m = pat.search(p.stem)\n",
    "        if m:\n",
    "            ids.append(int(m.group(1)))\n",
    "    if not ids:\n",
    "        return 0\n",
    "    ids_sorted = sorted(ids)\n",
    "    if ids_sorted[0] != 0 or ids_sorted[-1] != len(ids_sorted) - 1:\n",
    "        raise ValueError(\n",
    "            f\"obses episode indices not contiguous in {obs_dir} \"\n",
    "            f\"(min={ids_sorted[0]}, max={ids_sorted[-1]}, n={len(ids_sorted)})\"\n",
    "        )\n",
    "    return len(ids_sorted)\n",
    "\n",
    "def torch_load_safe(path: Path):\n",
    "    # Avoid mmap quirks; keep it simple for checks.\n",
    "    return torch.load(str(path), map_location=\"cpu\")\n",
    "\n",
    "def check_tensor_first_dim(path: Path, expected_n: int) -> bool:\n",
    "    \"\"\"Returns True if OK, False if mismatch. Prints details.\"\"\"\n",
    "    if not path.exists():\n",
    "        print(f\"    - {path.name}: (missing) [SKIP]\")\n",
    "        return True\n",
    "    try:\n",
    "        obj = torch_load_safe(path)\n",
    "    except Exception as e:\n",
    "        print(f\"    - {path.name}: could not load ({type(e).__name__}: {e})\")\n",
    "        return False\n",
    "\n",
    "    ok = True\n",
    "    if torch.is_tensor(obj):\n",
    "        if obj.ndim < 1:\n",
    "            print(f\"    - {path.name}: tensor has ndim<1 [WARN]\")\n",
    "        elif obj.shape[0] != expected_n:\n",
    "            print(f\"    - {path.name}: BAD first dim={obj.shape[0]} expected={expected_n}\")\n",
    "            ok = False\n",
    "        else:\n",
    "            print(f\"    - {path.name}: OK shape={tuple(obj.shape)}\")\n",
    "    elif isinstance(obj, dict):\n",
    "        # Some files are dicts; check any tensors inside that look episode-aligned.\n",
    "        found_any = False\n",
    "        for k, v in obj.items():\n",
    "            if torch.is_tensor(v) and v.ndim >= 1:\n",
    "                found_any = True\n",
    "                if v.shape[0] != expected_n:\n",
    "                    print(f\"    - {path.name}[{k!r}]: BAD first dim={v.shape[0]} expected={expected_n}\")\n",
    "                    ok = False\n",
    "                else:\n",
    "                    print(f\"    - {path.name}[{k!r}]: OK shape={tuple(v.shape)}\")\n",
    "        if not found_any:\n",
    "            print(f\"    - {path.name}: dict (no tensor fields checked) [INFO]\")\n",
    "    else:\n",
    "        print(f\"    - {path.name}: type={type(obj).__name__} (not checked) [INFO]\")\n",
    "\n",
    "    del obj\n",
    "    gc.collect()\n",
    "    return ok\n",
    "\n",
    "def check_split(split_dir: Path):\n",
    "    lengths = load_lengths(split_dir)\n",
    "    n = len(lengths)\n",
    "\n",
    "    print(f\"  split: {split_dir.name}\")\n",
    "    print(f\"    episodes (seq_lengths): {n:,} | minT={int(lengths.min())} maxT={int(lengths.max())} meanT={float(lengths.mean()):.2f}\")\n",
    "\n",
    "    if np.any(lengths <= 0):\n",
    "        raise ValueError(f\"{split_dir}: seq_lengths contains non-positive values\")\n",
    "    if np.any(~np.isfinite(lengths)):\n",
    "        raise ValueError(f\"{split_dir}: seq_lengths contains non-finite values\")\n",
    "\n",
    "    obs_dir = split_dir / \"obses\"\n",
    "    if obs_dir.exists():\n",
    "        obs_n = count_obs_eps(obs_dir)\n",
    "        print(f\"    obses/: {obs_n:,} episode_* files\")\n",
    "        if obs_n != n:\n",
    "            print(f\"    WARNING: obses count ({obs_n}) != seq_lengths count ({n})\")\n",
    "    else:\n",
    "        print(f\"    obses/: (not present)\")\n",
    "\n",
    "    # Explicit action/state checks\n",
    "    ok = True\n",
    "    ok &= check_tensor_first_dim(split_dir / ACTIONS_FNAME, n)\n",
    "    ok &= check_tensor_first_dim(split_dir / STATES_FNAME, n)\n",
    "    ok &= check_tensor_first_dim(split_dir / STATES_CONST_FNAME, n)\n",
    "\n",
    "    # Optional: scan other top-level tensor files and report episode-aligned ones\n",
    "    extra = sorted(list(split_dir.glob(\"*.pth\")) + list(split_dir.glob(\"*.pt\")))\n",
    "    extra = [p for p in extra if p.name not in (ACTIONS_FNAME, STATES_FNAME, STATES_CONST_FNAME)]\n",
    "    if extra:\n",
    "        print(f\"    other tensor files (top-level): {len(extra)}\")\n",
    "        for p in extra:\n",
    "            # only print if it matches N; otherwise just note mismatch\n",
    "            try:\n",
    "                obj = torch_load_safe(p)\n",
    "                if torch.is_tensor(obj) and obj.ndim >= 1:\n",
    "                    tag = \"OK\" if obj.shape[0] == n else \"mismatch\"\n",
    "                    print(f\"    - {p.name}: {tag} shape={tuple(obj.shape)}\")\n",
    "                else:\n",
    "                    print(f\"    - {p.name}: type={type(obj).__name__} [INFO]\")\n",
    "                del obj\n",
    "            except Exception as e:\n",
    "                print(f\"    - {p.name}: could not load ({type(e).__name__}: {e})\")\n",
    "            gc.collect()\n",
    "\n",
    "    print(\"    RESULT:\", \"OK\\n\" if ok else \"HAS ISSUES (see above)\\n\")\n",
    "\n",
    "for root in SUBSET_DIRS:\n",
    "    print(f\"\\n=== Checking subset: {root} ===\")\n",
    "    if not root.exists():\n",
    "        print(\"  MISSING (dir does not exist)\")\n",
    "        continue\n",
    "    for split in (\"train\", \"val\"):\n",
    "        split_dir = root / split\n",
    "        if not split_dir.exists():\n",
    "            print(f\"  MISSING split dir: {split_dir}\")\n",
    "            continue\n",
    "        check_split(split_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino_wm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
